
0. Problem from:
http://codegolf.stackexchange.com/questions/36921/mastermind-horse-battery-staple
Given a dictionary (lower case alpha only), and a collection of pass phrases 
(3 dictionary words, separated by a single space each)  and a leaky password
matcher (ala hangman) which returns a count of positions, and characters 
matched, guess the passphrase with the least number of match attempts.

Solution:
1. Process the dictionary - read the file, and collect stats.
   1. Maximum word length
   2. Minimum word length
   3. distribution of characters across words

2. Try the following for spaces:
   Use a series of test strings, each with half the values being a space, 
   and the other half a ZPC (Zero Probability Character).  Track all 
   possible pairs of positions that could be spaces, till we have only
   1 pair left.  This gives the positions of the spaces, and the lengths
   of the first 2 words.

   In the first phase, the phrase is generated independent of the tuples.  In
   the second phase, this is based on the potential pairs, and could be as
   brute force as trying to confirm / eliminate 1 pair at a time.

3. Guess each word separately
   Word 1: Get a sub dictionary with the known word length
	   Now repeat till done!
	       Guess a word, Find exact position matches
	       Reduce dictionary to only words that have as many pos matches

   Word 2: Similar to word 1

   Word 3: We dont know word length
	   Use the responses from previous guesses to reduce the dictionary
	   as much as possible.
	   Now repeat till done!
	       Guess a word, Find exact position matches
	       Reduce dictionary to only words that have as many pos matches

4. Statistics from this approach
   Blanks : 6862
   Word 1 words : 5960
   Word 2 words : 5907
   Word 3 words : 2953
   Phrase Confirm : 1000
   TOTAL  : 21682

   Dict for word 1 : 1157
   Dict for word 2 : 1172
   Dict for word 3 : 94

5. Caveats - I have not paid attention to memory management in this!


It is possible to reduce this further by the following

1. Use a query string of ([a-z] X maxphraselength + 2) in the first attempt.
This will return the total length of the string as the character match.
If the first (maxlength) characters are the most frequent ones, the position 
count is the actual number of those chars.

1.a Knowing the length reduces the tests in the space finding phase.  The 
    additinal query pays for itself in this itself, but not much more.
1.b.Knowing the length also identifies the length of word 3.  This results 
    in significant savings (upto 1.5 attempts) in while solving for word 3.
    Additional savings is found in the reduced CPU consumption since the 
    cost of reducing the dictionary size comes down!

2. Insert additional text in the space finding phase.  This can be used to
   count low frequency words, and reduce the dictionary size for words 1
   and 2 as well. This reduces the dictionary for words 1 and 2 by about 
   15 - 20%, and for word3 as well.
   This saves another attempt overall. 

The results are:
    Blanks : 6548
    Word 1 words : 5718
    Word 2 words : 5722
    Word 3 words : 1289
    Phrase Confirm : 1000
    TOTAL  : 20277

    Dict for word 1 : 915
    Dict for word 2 : 927
    Dict for word 3 : 916
    Dict for word 3(constrained) : 9

There may be some improvements in the way the words are guessed, which could
result in an additional improvement of 2 more attempt overall, but I think
this approach stops paying dividends at that time.

Meanwhile Roy figured out a way to do the entire phrase (and not words) which 
breaks the 14 attempts mark!  Trying to see if we can do better!
The first phase (length finding and space finding) result in 3 dictionaries
whose size multiplies to about 800M.  It is clearly infeasible to create
that in memory, and then apply mastermind type of techniques to guess.

We have to look at 
a. Reducing the dictionary size.
   Looking for additional information which can reduce the dictionary size
   while searching for spaces leads to reduction in dictionary size per word
   (comes to about 550 to 600).  Applying 1 word reduction brings that down 
   to about 170.  Hence, after ~ 10 guesses, we are down to 5M combinations.

b. Applying the information we have.
   A word match results in position and character match information.  The 
   position information is used to refine the dictionary.  However, it is
   also possible to use the character match information for all 3 words.

c. Improving the word guess, so that the dictionary shrinks faster.
   Current strategy is to use a synthetic word the first time, and 
   then use the word with the most character diversity.

Using these strategies - we bring the number of attempts down to about 18000.
    Blanks : 6548
    Word 1 words : 5025
    Word 2 words : 5016
    Word 3 words : 470
    Phrase Confirm : 1000
    TOTAL  : 18059

This I think is about the limit of of word based techniques. 

However, with phrase based techniques, we should be able to do much better.


With Phrase based matching, we are down to 11383 attempts!  It is expensive
in terms of compute!  I also dont like the code structure!

This is how I am doing it:
1. Measure the length of the phrase - using a string with all 26 characters
   max times (max = 3 * maxwordlen + 2) and 2 spaces.  First maxlen characters
   are the most frequent in the dictionary i.e. e
2. Use a binary sieve kind of strategy to identify the spaces - do a set number
   of attempts, and identify potential pairs of spaces.
   Create specific test strings to reduce to a single pair.
3. In parallel, append 'crafted' test strings to get more information about 
   the phrase.  The current strategy is as follows:
   a. Use characters in order of their frequency in the dictionary.
   b. We already know the count for the most frequent
   c. 1st Test string = next 5 characters.  This gives us the count of these
      characters in the phrase.
   d. next 3 test strings = next 5 characters each, covering a total of 20 
      characters in 4 attempts in addition to the first 1 char.
      This gives us the count for these last 5 characters as well.
      sets with 0 count are great for reducing the dictionary size!
   e. Now for the previous test that had the least, non-zero counts, split the 
      string into 2, and use 1 for testing.  The resulting count tells us about
      the other split as well.
   f. Now repeat tests with characters (0-based), 
       1,6,11,16,21
       2,7,12,17,22
       3,8,13,18,23
       4,9,14,19,24
       This should give us 5,10,15,20,25
    g. After this, the next set of test strings are all 1 character long.
       though we dont expect to get so many tries!
4. Once the spaces are identified, use the constraints so far (as many tests
   as could be done in these attempts) to reduce the size of the dictionary.
   Also create 3 sub dictionaries, 1 for each word.
5. Now create a synthetic guess word for each dictionary, and test it.  
   Use these results to reduce the individual dictionary sizes.  
   Decorate this with test characters as well (after the length) to get more
   constraints on the phrase!
6. Do one more round of guessing for words 1, 2 and 3.  This brings the 
   dictionary to a manageable size.  Perform a cross product, applying all 
   constraints as before to create a phrase dictionary.
7. Solve for the phrase dictionary through a series of guesses - this time
   using both position and character match information.
8. This approach brings us to under 13500 attempts:
    Matcher Statistics
    ------------------
    Length : 1000
    Spaces : 6375
    Word 1 : 1996
    Word 2 : 1995
    Word 3 : 1992
    Phrase : 132
    TOTAL  : 13490

    Dictionary Statistics
    word 0	6517
    word 1	780	221	91
    word 2	791	232	92
    word 3	774	232	95
    word 4	2	2

    Solution time: 4 minutes on my macbook pro.

9. Going back to step 5 and 6 - if we solve word1 in 2 passes (1 synthetic)
   word 2 in 1 pass (synthetic), and dont reduce dictionary for word3, we
   get a computationally harder, but still tractable solution, which looks like:

    Matcher Statistics
    ------------------
    Length : 1000
    Spaces : 6375
    Word 1 : 1996
    Word 2 : 999
    Phrase : 1013
    TOTAL  : 11383

    Dictionary Statistics
    word 0	6517
    word 1	780	221	92
    word 2	791	233
    word 3	772
    word 4	186	20	4	2

    Solution time: 20 minutes on my macbook pro.

