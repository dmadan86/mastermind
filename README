(This readme is more like a log of changes made to solve the probem!)
0. Problem from:
http://codegolf.stackexchange.com/questions/36921/mastermind-horse-battery-staple
Given a dictionary (lower case alpha only), and a collection of pass phrases 
(3 dictionary words, separated by a single space each)  and a leaky password
matcher (ala hangman) which returns a count of positions, and characters 
matched, guess the passphrase with the least number of match attempts.

Solution:
1. Process the dictionary - read the file, and collect stats.
   1. Maximum word length
   2. Minimum word length
   3. distribution of characters across words

2. Try the following for spaces:
   Use a series of test strings, each with half the values being a space, 
   and the other half a ZPC (Zero Probability Character).  Track all 
   possible pairs of positions that could be spaces, till we have only
   1 pair left.  This gives the positions of the spaces, and the lengths
   of the first 2 words.

   In the first phase, the phrase is generated independent of the tuples.  In
   the second phase, this is based on the potential pairs, and could be as
   brute force as trying to confirm / eliminate 1 pair at a time.

3. Guess each word separately
   Word 1: Get a sub dictionary with the known word length
	   Now repeat till done!
	       Guess a word, Find exact position matches
	       Reduce dictionary to only words that have as many pos matches

   Word 2: Similar to word 1

   Word 3: We dont know word length
	   Use the responses from previous guesses to reduce the dictionary
	   as much as possible.
	   Now repeat till done!
	       Guess a word, Find exact position matches
	       Reduce dictionary to only words that have as many pos matches

4. Statistics from this approach
   Blanks : 6862
   Word 1 words : 5960
   Word 2 words : 5907
   Word 3 words : 2953
   Phrase Confirm : 1000
   TOTAL  : 21682

   Dict for word 1 : 1157
   Dict for word 2 : 1172
   Dict for word 3 : 94

5. Caveats - I have not paid attention to memory management in this!


It is possible to reduce this further by the following

1. Use a query string of ([a-z] X maxphraselength + 2) in the first attempt.
This will return the total length of the string as the character match.
If the first (maxlength) characters are the most frequent ones, the position 
count is the actual number of those chars.

1.a Knowing the length reduces the tests in the space finding phase.  The 
    additinal query pays for itself in this itself, but not much more.
1.b.Knowing the length also identifies the length of word 3.  This results 
    in significant savings (upto 1.5 attempts) in while solving for word 3.
    Additional savings is found in the reduced CPU consumption since the 
    cost of reducing the dictionary size comes down!

2. Insert additional text in the space finding phase.  This can be used to
   count low frequency words, and reduce the dictionary size for words 1
   and 2 as well. This reduces the dictionary for words 1 and 2 by about 
   15 - 20%, and for word3 as well.
   This saves another attempt overall. 

The results are:
    Blanks : 6548
    Word 1 words : 5718
    Word 2 words : 5722
    Word 3 words : 1289
    Phrase Confirm : 1000
    TOTAL  : 20277

    Dict for word 1 : 915
    Dict for word 2 : 927
    Dict for word 3 : 916
    Dict for word 3(constrained) : 9

There may be some improvements in the way the words are guessed, which could
result in an additional improvement of 2 more attempt overall, but I think
this approach stops paying dividends at that time.

Meanwhile Roy figured out a way to do the entire phrase (and not words) which 
breaks the 14 attempts mark!  Trying to see if we can do better!
The first phase (length finding and space finding) result in 3 dictionaries
whose size multiplies to about 800M.  It is clearly infeasible to create
that in memory, and then apply mastermind type of techniques to guess.

We have to look at 
a. Reducing the dictionary size.
   Looking for additional information which can reduce the dictionary size
   while searching for spaces leads to reduction in dictionary size per word
   (comes to about 550 to 600).  Applying 1 word reduction brings that down 
   to about 170.  Hence, after ~ 10 guesses, we are down to 5M combinations.

b. Applying the information we have.
   A word match results in position and character match information.  The 
   position information is used to refine the dictionary.  However, it is
   also possible to use the character match information for all 3 words.

c. Improving the word guess, so that the dictionary shrinks faster.
   Current strategy is to use a synthetic word the first time, and 
   then use the word with the most character diversity.

Using these strategies - we bring the number of attempts down to about 18000.
    Blanks : 6548
    Word 1 words : 5025
    Word 2 words : 5016
    Word 3 words : 470
    Phrase Confirm : 1000
    TOTAL  : 18059

This I think is about the limit of of word based techniques. 

However, with phrase based techniques, we should be able to do much better.

With Phrase based matching, we are down to 11383 attempts!  It is expensive
in terms of compute!  I also dont like the code structure!

This is how I am doing it:
1. Measure the length of the phrase - using a string with all 26 characters
   max times (max = 3 * maxwordlen + 2) and 2 spaces.  First maxlen characters
   are the most frequent in the dictionary i.e. e
2. Use a binary sieve kind of strategy to identify the spaces - do a set number
   of attempts, and identify potential pairs of spaces.
   Create specific test strings to reduce to a single pair.
3. In parallel, append 'crafted' test strings to get more information about 
   the phrase.  The current strategy is as follows:
   a. Use characters in order of their frequency in the dictionary.
   b. We already know the count for the most frequent
   c. 1st Test string = next 5 characters.  This gives us the count of these
      characters in the phrase.
   d. next 3 test strings = next 5 characters each, covering a total of 20 
      characters in 4 attempts in addition to the first 1 char.
      This gives us the count for these last 5 characters as well.
      sets with 0 count are great for reducing the dictionary size!
   e. Now for the previous test that had the least, non-zero counts, split the 
      string into 2, and use 1 for testing.  The resulting count tells us about
      the other split as well.
   f. Now repeat tests with characters (0-based), 
       1,6,11,16,21
       2,7,12,17,22
       3,8,13,18,23
       4,9,14,19,24
       This should give us 5,10,15,20,25
    g. After this, the next set of test strings are all 1 character long.
       though we dont expect to get so many tries!
4. Once the spaces are identified, use the constraints so far (as many tests
   as could be done in these attempts) to reduce the size of the dictionary.
   Also create 3 sub dictionaries, 1 for each word.
5. Now create a synthetic guess word for each dictionary, and test it.  
   Use these results to reduce the individual dictionary sizes.  
   Decorate this with test characters as well (after the length) to get more
   constraints on the phrase!
6. Do one more round of guessing for words 1, 2 and 3.  This brings the 
   dictionary to a manageable size.  Perform a cross product, applying all 
   constraints as before to create a phrase dictionary.
7. Solve for the phrase dictionary through a series of guesses - this time
   using both position and character match information.
8. This approach brings us to under 13500 attempts:
    Matcher Statistics
    ------------------
    Length : 1000
    Spaces : 6375
    Word 1 : 1996
    Word 2 : 1995
    Word 3 : 1992
    Phrase : 132
    TOTAL  : 13490

    Dictionary Statistics
    word 0	6517
    word 1	780	221	91
    word 2	791	232	92
    word 3	774	232	95
    phrase	2	2

    Solution time: 4 minutes on my macbook pro.

VARIANTS
.  Going back to step 5 and 6 - if we solve word1 in 2 passes (1 synthetic)
   word 2 in 1 pass (synthetic), and dont reduce dictionary for word3, we
   get a computationally harder, but still tractable solution, which looks like:

    Matcher Statistics
    ------------------
    Length : 1000
    Spaces : 6375
    Word 1 : 1996
    Word 2 : 999
    Phrase : 1013
    TOTAL  : 11383

    Dictionary Statistics
    word 0	6517
    word 1	780	221	92
    word 2	791	233
    word 3	772
    phrase	186	20	4	2

    Solution time: 20 minutes on my macbook pro.

.   Going back to step 5 & 6 - we further reduce the solving of words to 
    1 pass on words 1 & 2, it reduces the number of attempts at the cost
    of a larger dictionary and compute time like so:

    Length : 1000
    Spaces : 6375
    Word 1 : 1000
    Word 2 : 999
    Phrase : 1615
    TOTAL  : 10989

    word 0	6517
    word 1	780	222
    word 2	792	233
    word 3	774
    phrase	2040	116	13	3

    Solution time: 46 minutes on my macbook pro.
    This is the last one I did for submitting to the codegolf site

.   Just as an experiment, I went the other way - to solving words 1 & 2
    completely, before attacking word 3 (the way I did my original solution
    except that now I had improved code, analytics and probing strategies).
    This is what I got:
    Length : 1000
    Spaces : 6375
    Word 1 : 5541
    Word 2 : 5600
    Word 3 : 34
    TOTAL  : 18550

    word 0	6517
    word 1	780	330	124	50	20	9	4	...
    word 2	793	329	127	51	21	9	4	...
    word 3	776	1

    Solution time: 6 seconds on my macbook pro.

Summarized Learnings:
.   I think this approach can be taken further to reduce the word solving 
    attempts, but that only increases the compute cost.
    It is interesting to think of this problem as having 2 parts:
       The most information that you can gather with a single attempt
       The best way to use that to reduce the number of attempts (and compute cost)

    Learnings and techniques used:
    . Single query to get the length of the phrase - great!
    . Same query to get the count of any 1 character - awesome!
      (Thanks to Roy who got this first)
    . Space finding as a constraint optimization - interesting
    . Using constraints for phrases in a modified way for words
    . Tacking on additional probe payload in the attempts such that the 
      immediate usefulness of the attempts is not impacted.

     There is other interesting learnings about how to craft attempts - I 
     believe that identifying the low frequency characters helps to reduce
     the problem size, including word dictionaries.  My hypothese has been that 
     the best probe strategy is to get something like:
	 e : 3
	 iarsn : 11
	 tolcd : 7
	 umpgh : 0
	 byfvk : 3
	 wzxjq : 4

     The winner of this challenge seems to believe the opposite, getting the
     count of single characters from the high frequency side!

Wrote this too soon!
    When I wrote it, the top contender was Roy Van Rijn with 10836.

    The winner of the challenge - Peter Weistroffer - went onto win the 
    challenge with the following strategy (9447):
    phase 1: 1st guess: Get length, and test for odd/even spaces.
    phase 2: Look for spaces, and test multiple chars  The chars are 
             selected to always include 2 out of e/i/a/s, and such that 
	     half the phrase length is expected return.
	     (repeat till spaces are known)
    phase 3: Select frequent characters for the main query, and continue 
             with the frequency collection for the additional string
    phase 4: Solve the mastermind puzzle - 2 strategies, 1 for large, 
             and 1 for small set size.

    Code is in scala, and only 350 lines!
    For speed, character counts (top 7) were put into a long, and comparision
    was used.  The other counts were ignored to improve speed.

Looking at some basic counting - before digging into a new solution:
Word Size Distribution in the dictionary
     1: 5
     2: 31
     3: 146
     4: 473
     5: 944
     6: 1378
     7: 1615
     8: 1584
     9: 1313
    10: 1023
    11: 644
    12: 405
    13: 229
    14: 117
    15: 55
    16: 18
    17: 14
    18: 4
    20: 1
    21: 1

    // We know nothing but the dictionary - what is the potential size of 
    // the phrase dictionary
544,433,496,623		takes 39 bits to represent

    // Assuming that we know wordlength, but not the locations of the 
    // spaces, what is the potential size of the phrase dictionary
 29,627,753,185		takes 35 bits to represent

    // Now that we know wordlength, and the locations of the spaces, what is 
    // the potential size of the phrase dictionary
  1,588,088,745		takes 31 bits to represent

    // Now that we know wordlength, the locations of the spaces and some 
    // constraints on the words, what is the potential size of the phrase 
    // dictionary  (These are the constraints used in my submission to 
    // codegolf, and applied at the word level)
    610,831,087		takes 30 bits to represent

    Average Dictionary sizes are: 780, 793, 776

    This took 7.4 attempts to get to, which is a problem, because we have only
    reduced entropy by 9 bits in 7 attempts - of which the first attempt itself
    gave us 4 bits reduction!

Current State (Sep 14, 2014)
    Reworked this to be similar to Weistroff's process:
    phase 1: 1st guess: Get length, and test for odd/even spaces.
    phase 2: Look for spaces.  
             2 sub-phases: a where the number of possible pairs is large, 
	     Here, we partition the unknown places based on what we know
	     trying to get responses (0,1,2) to be in proportion (1:2:1).
	     b. When the number of pairs is small < 8, evaluate all possible
	     partitions and select the best.
	     Appears to be performing poorer than a previous implementation,
	     where we got the space identification in 5347 attempts.
	     Look for character counts: 
		 divide characters by quartiles based on their contribution
		 to the dictionary words: q1, q2, q3, q4
		 attempt 1: characters from q1, q2
		 attempt 2: characters from q2, q3
		 attempt 3: (intersection of 1&2) characters from q2
		 Now we know counts from each quarter!
		 attempt 4: find collection with lowest count, partition it into
		 2 equal halves, and use one of them.
		 attempt 5: Odd (in frequency rank)
		 attempt 6: 0,1, 4,5, 8,9, ...
		 attempt 7: intersection of 5,6
             Broadly: Test a group that accounts for 50% of characters in dict.
	     The result tells us the count for the tested set, and the rest!
	     When you intersect 2 consecutive tests, you know all 4 quartiles
	     individually in 3 tests.  This should help reduce dict size.
	     Breaking up the least count set into 2 should help reduce dict
	     size even more.

	     (repeat till spaces are known)
    phase 3: Reduce dictionary based on constraints discovered in phase 2.
             Create 3 sub dictionaries (1 for each word).
	     Do a cross product to get all possible phrases.
	     Apply all known information so far to reduce this.
	     Now - apply mastermind solving techniques to solve.
The results are:
    Spaces : 6438
    Phrase : 3440
    TOTAL  : 9878

    word 0	6971
    word 1	841
    word 2	854
    word 3	835
    phrase	1000252	38307	1507	112	11	3	2
		20	16	11	7	4	2	1

Going back to our thinking about the information - it looks like that the 
phrase dictionary is about a million (20 bits) long on average, and reduces
to a solution in 3.4 attempts, wih a max of 7!  This is a reduction of 4
bits per guess.

This (9878) is still worse than the winning solution - 9447 and took 10.5 hours on my macbook pro!

Known improvements:
a. Space Finding - can be improved
b. Space finding phase - different strategies could be tried
